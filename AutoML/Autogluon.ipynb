{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import BallTree\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.tabular import FeatureMetadata\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_setting(seed=1004) :\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4383/3094210263.py:1: DtypeWarning: Columns (16,17,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('train.csv')\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "subway = pd.read_csv('subway_feature.csv')\n",
    "bus = pd.read_csv('bus_feature.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entire_Preprocessing(df) :\n",
    "    # 문자열 컬럼만 찾아서 좌우 공백 제거\n",
    "    df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
    "\n",
    "    df = df.drop(columns=['본번', '부번', '시군구', 'k-전화번호', 'k-팩스번호',\n",
    "                          'k-홈페이지', '고용보험관리번호', 'k-등록일자', 'k-수정일자',\n",
    "                          '관리비 업로드', '단지소개기존clob'])\n",
    "\n",
    "    # 군집화\n",
    "    ## 카카오 API 호출 함수\n",
    "    def get_coords_kakao(address, api_key):\n",
    "        url = \"https://dapi.kakao.com/v2/local/search/address.json\"\n",
    "        headers = {\"Authorization\": f\"KakaoAK {api_key}\"}\n",
    "        params = {\"query\": address}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        result = response.json()\n",
    "        \n",
    "        try:\n",
    "            x = float(result['documents'][0]['x'])\n",
    "            y = float(result['documents'][0]['y'])\n",
    "            return x, y\n",
    "        except IndexError:\n",
    "            return None, None\n",
    "\n",
    "    ## 도로명을 기반으로 좌표X와 좌표Y를 받아옴 (결측치에 한해서)\n",
    "    def fill_missing_coords(row):\n",
    "        if pd.isna(row['좌표X']) or pd.isna(row['좌표Y']):\n",
    "            coords = roadname_to_coords.get(row['도로명'])\n",
    "            if coords:\n",
    "                return pd.Series(coords)\n",
    "        return pd.Series([row['좌표X'], row['좌표Y']])\n",
    "    \n",
    "    roadname_to_coords = {}\n",
    "    unique_roads = df.loc[df[['좌표X', '좌표Y']].isnull().any(axis=1), '도로명'].dropna().unique()\n",
    "\n",
    "    api_key = '13b7b7a0b7a853100b56c56f19f6bc24'\n",
    "\n",
    "    for road in tqdm(unique_roads) :\n",
    "        x, y = get_coords_kakao(road, api_key)\n",
    "        if x is not None and y is not None :\n",
    "            roadname_to_coords[road] = (x, y)\n",
    "\n",
    "    df[['좌표X', '좌표Y']] = df.apply(fill_missing_coords, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8441/8441 [10:06<00:00, 13.92it/s]\n"
     ]
    }
   ],
   "source": [
    "df = Entire_Preprocessing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['등기신청일자', '해제사유발생일'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip(군집화, 버스, 지하철거리, 금리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dst78/anaconda3/envs/Deep/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator KMeans from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/dst78/anaconda3/envs/Deep/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but KMeans was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 군집화 & 버스, 지하철 거리\n",
    "df = df.dropna(subset=['좌표X', '좌표Y'])\n",
    "kmeans = joblib.load('kmeans_model.pkl')\n",
    "\n",
    "df['cluster'] = kmeans.predict(df[['좌표X', '좌표Y']])\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    lat1 = np.radians(lat1)[:, np.newaxis]\n",
    "    lon1 = np.radians(lon1)[:, np.newaxis]\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "apt_coords_rad = np.radians(df[['좌표Y', '좌표X']].to_numpy())\n",
    "bus_coords_rad = np.radians(bus[['Y좌표', 'X좌표']].to_numpy())\n",
    "subway_coords_rad = np.radians(subway[['위도', '경도']].to_numpy())\n",
    "\n",
    "bus_tree = BallTree(bus_coords_rad, metric='haversine')\n",
    "subway_tree = BallTree(subway_coords_rad, metric='haversine')\n",
    "\n",
    "dist_bus_rad, _ = bus_tree.query(apt_coords_rad, k=1)\n",
    "dist_sub_rad, _ = subway_tree.query(apt_coords_rad, k=1)\n",
    "\n",
    "df['closest_bus'] = dist_bus_rad.flatten() * 6371.0\n",
    "df['closest_sub'] = dist_sub_rad.flatten() * 6371.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 금리\n",
    "df['계약(연)'] = df['계약년월'] // 100\n",
    "df['계약(월)'] = df['계약년월'] % 100\n",
    "df = df.drop(columns=['계약년월'])\n",
    "\n",
    "df['연_월'] = pd.PeriodIndex(year=df['계약(연)'], month=df['계약(월)'], freq='M')\n",
    "\n",
    "rates = {\n",
    "    \"2025-02\": 2.75, \"2024-11\": 3.00, \"2024-10\": 3.25, \"2023-01\": 3.50,\n",
    "    \"2022-11\": 3.25, \"2022-10\": 3.00, \"2022-08\": 2.50, \"2022-07\": 2.25,\n",
    "    \"2022-05\": 1.75, \"2022-04\": 1.50, \"2022-01\": 1.25, \"2021-11\": 1.00,\n",
    "    \"2021-08\": 0.75, \"2020-05\": 0.50, \"2020-03\": 0.75, \"2019-10\": 1.25,\n",
    "    \"2019-07\": 1.50, \"2018-11\": 1.75, \"2017-11\": 1.50, \"2016-06\": 1.25,\n",
    "    \"2015-06\": 1.50, \"2015-03\": 1.75, \"2014-10\": 2.00, \"2014-08\": 2.25,\n",
    "    \"2013-05\": 2.50, \"2012-10\": 2.75, \"2012-07\": 3.00, \"2011-06\": 3.25,\n",
    "    \"2011-03\": 3.00, \"2011-01\": 2.75, \"2010-11\": 2.50, \"2010-07\": 2.25,\n",
    "    \"2009-02\": 2.00, \"2009-01\": 2.50, \"2008-12\": 3.00, \"2008-11\": 4.00,\n",
    "    \"2008-10\": 5.00, \"2008-08\": 5.25, \"2007-08\": 5.00, \"2007-07\": 4.75,\n",
    "    \"2006-08\": 4.50\n",
    "}\n",
    "\n",
    "rate_changes = pd.DataFrame({\n",
    "    '연_월' : list(rates.keys()),\n",
    "    '금리' : list(rates.values())\n",
    "})\n",
    "\n",
    "rate_changes['연_월'] = pd.PeriodIndex(rate_changes['연_월'], freq='M')\n",
    "rate_changes = rate_changes.sort_values('연_월').reset_index(drop=True)\n",
    "\n",
    "rate_changes['start_month'] = rate_changes['연_월']\n",
    "rate_changes['end_month'] = rate_changes['연_월'].shift(-1) - 1\n",
    "rate_changes.at[rate_changes.index[-1], 'end_month'] = pd.Period('2099-12', freq='M')\n",
    "\n",
    "def assign_rate(contract_period):\n",
    "    matched = rate_changes[(rate_changes['start_month'] <= contract_period) & (rate_changes['end_month'] >= contract_period)]\n",
    "    if not matched.empty:\n",
    "        return matched.iloc[0]['금리']\n",
    "    return None\n",
    "    \n",
    "df['금리'] = df['연_월'].apply(assign_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col != 'target']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.11.5\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "CPU Count:          24\n",
      "Memory Avail:       16.25 GB / 23.36 GB (69.6%)\n",
      "Disk Space Avail:   552.64 GB / 1006.85 GB (54.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-05-15 13:29:00,496\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/dst78/AI_BootCamp/Apart_predict/autogluon_models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Beginning AutoGluon training ... Time limit = 898s\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m AutoGluon will save models to \"/home/dst78/AI_BootCamp/Apart_predict/autogluon_models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Train Data Rows:    974783\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Train Data Columns: 46\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tAvailable Memory:                    14278.75 MB\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tTrain Data (Original)  Memory Usage: 1145.50 MB (8.0% of available memory)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tWarning: Data size prior to feature transformation consumes 8.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Cannot interpret 'period[M]' as a data type\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting DatetimeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tUnused Original Features (Count: 1): ['연_월']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('period[M]', []) : 1 | ['연_월']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('float', [])                      : 24 | ['전용면적(㎡)', '계약일', '층', '건축년도', '해제사유발생일', ...]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('int', [])                        :  1 | ['cluster']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('object', [])                     : 17 | ['번지', '아파트명', '도로명', '거래유형', '중개사소재지', ...]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('object', ['datetime_as_object']) :  3 | ['k-사용검사일-사용승인일', '단지승인일', '단지신청일']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('category', [])             : 16 | ['번지', '아파트명', '도로명', '거래유형', '중개사소재지', ...]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('float', [])                : 23 | ['전용면적(㎡)', '계약일', '층', '건축년도', '해제사유발생일', ...]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('int', [])                  :  1 | ['cluster']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('int', ['bool'])            :  2 | ['k-135㎡초과', '사용허가여부']\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t\t('int', ['datetime_as_int']) : 15 | ['k-사용검사일-사용승인일', 'k-사용검사일-사용승인일.year', 'k-사용검사일-사용승인일.month', 'k-사용검사일-사용승인일.day', 'k-사용검사일-사용승인일.dayofweek', ...]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t7.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t45 features in original data used to generate 57 features in processed data.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tTrain Data (Processed) Memory Usage: 308.64 MB (2.2% of available memory)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Data preprocessing and feature engineering runtime = 8.05s ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'extra_trees': False, 'ag_args': {'name_suffix': 'Default'}}],\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 267.02s of the 890.3s of remaining time.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.61% memory usage per fold, 62.44%/80.00% total).\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=15.61%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=12732)\u001b[0m [1000]\tvalid_set's rmse: 8542.69\n",
      "\u001b[36m(_ray_fit pid=12729)\u001b[0m [1000]\tvalid_set's rmse: 7707.48\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12732)\u001b[0m [2000]\tvalid_set's rmse: 7707.43\n",
      "\u001b[36m(_ray_fit pid=12731)\u001b[0m [2000]\tvalid_set's rmse: 7419.05\n",
      "\u001b[36m(_ray_fit pid=12732)\u001b[0m [3000]\tvalid_set's rmse: 7355.79\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12730)\u001b[0m [3000]\tvalid_set's rmse: 7018.86\n",
      "\u001b[36m(_ray_fit pid=12731)\u001b[0m [3000]\tvalid_set's rmse: 7018.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=12732)\u001b[0m \tRan out of time, early stopping on iteration 3380. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=12732)\u001b[0m \t[3380]\tvalid_set's rmse: 7267.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=13468)\u001b[0m [1000]\tvalid_set's rmse: 7948.89\n",
      "\u001b[36m(_ray_fit pid=13538)\u001b[0m [1000]\tvalid_set's rmse: 8331.61\n",
      "\u001b[36m(_ray_fit pid=13468)\u001b[0m [2000]\tvalid_set's rmse: 7168.61\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13537)\u001b[0m [2000]\tvalid_set's rmse: 7129.41\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=13468)\u001b[0m \tRan out of time, early stopping on iteration 2864. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13468)\u001b[0m \t[2864]\tvalid_set's rmse: 6861.4\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/20 [04:23<?, ?it/s]\n",
      "\u001b[36m(_ray_fit pid=13594)\u001b[0m \tRan out of time, early stopping on iteration 2648. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13594)\u001b[0m \t[2648]\tvalid_set's rmse: 6870.15\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitted model: LightGBMXT_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-6962.2446\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t263.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t204.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Hyperparameter tuning model: LightGBMDefault_BAG_L1 ... Tuning model for up to 267.02s of the 625.98s of remaining time.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.17% memory usage per fold, 64.70%/80.00% total).\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=16.17%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=14342)\u001b[0m [1000]\tvalid_set's rmse: 6553.86\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14341)\u001b[0m [1000]\tvalid_set's rmse: 6810.3\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14342)\u001b[0m [2000]\tvalid_set's rmse: 6106.01\n",
      "\u001b[36m(_ray_fit pid=14338)\u001b[0m [2000]\tvalid_set's rmse: 6248.11\n",
      "\u001b[36m(_ray_fit pid=14341)\u001b[0m [2000]\tvalid_set's rmse: 6413.23\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=14341)\u001b[0m \tRan out of time, early stopping on iteration 2360. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=14341)\u001b[0m \t[2360]\tvalid_set's rmse: 6345.02\n",
      "\u001b[36m(_ray_fit pid=14336)\u001b[0m \tRan out of time, early stopping on iteration 2789. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=14336)\u001b[0m \t[2789]\tvalid_set's rmse: 5831.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15053)\u001b[0m [1000]\tvalid_set's rmse: 6487.25\n",
      "\u001b[36m(_ray_fit pid=15142)\u001b[0m [1000]\tvalid_set's rmse: 6601.93\n",
      "\u001b[36m(_ray_fit pid=15136)\u001b[0m [1000]\tvalid_set's rmse: 6404.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15053)\u001b[0m [2000]\tvalid_set's rmse: 6070.57\n",
      "\u001b[36m(_ray_fit pid=15142)\u001b[0m [2000]\tvalid_set's rmse: 6174.18\n",
      "\u001b[36m(_ray_fit pid=15136)\u001b[0m [2000]\tvalid_set's rmse: 5948.55\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15053)\u001b[0m [3000]\tvalid_set's rmse: 5894.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15053)\u001b[0m \tRan out of time, early stopping on iteration 3253. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15053)\u001b[0m \t[3253]\tvalid_set's rmse: 5865\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/20 [04:18<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitted model: LightGBMDefault_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-5945.9836\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t258.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t180.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 366.80s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=15267)\u001b[0m \tRan out of time, early stopping on iteration 2892. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15267)\u001b[0m \t[2892]\tvalid_set's rmse: 5624.7\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tEnsemble Weights: {'LightGBMDefault_BAG_L1/T1': 0.875, 'LightGBMXT_BAG_L1/T1': 0.125}\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-5923.1781\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t0.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Hyperparameter tuning model: LightGBMXT_BAG_L2 ... Tuning model for up to 164.94s of the 366.48s of remaining time.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.02% memory usage per fold, 68.08%/80.00% total).\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.02%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15942)\u001b[0m [1000]\tvalid_set's rmse: 6477.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15943)\u001b[0m \tRan out of time, early stopping on iteration 1768. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=15943)\u001b[0m \t[1768]\tvalid_set's rmse: 6287.64\n",
      "\u001b[36m(_ray_fit pid=15942)\u001b[0m \tRan out of time, early stopping on iteration 1754. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=15942)\u001b[0m \t[1753]\tvalid_set's rmse: 6289.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=16471)\u001b[0m [1000]\tvalid_set's rmse: 6595.63\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=16468)\u001b[0m \tRan out of time, early stopping on iteration 1753. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16468)\u001b[0m \t[1753]\tvalid_set's rmse: 6548.45\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/20 [02:20<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitted model: LightGBMXT_BAG_L2/T1 ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-6386.1365\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t140.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t37.49s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Hyperparameter tuning model: LightGBMDefault_BAG_L2 ... Tuning model for up to 164.94s of the 224.98s of remaining time.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\u001b[36m(_ray_fit pid=16471)\u001b[0m \tRan out of time, early stopping on iteration 1837. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16471)\u001b[0m \t[1837]\tvalid_set's rmse: 6351.86\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.04% memory usage per fold, 68.15%/80.00% total).\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.04%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17149)\u001b[0m [1000]\tvalid_set's rmse: 5826.95\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17520)\u001b[0m [1000]\tvalid_set's rmse: 6009.86\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17149)\u001b[0m \tRan out of time, early stopping on iteration 2399. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17149)\u001b[0m \t[2399]\tvalid_set's rmse: 5786.39\n",
      "\u001b[36m(_ray_fit pid=17147)\u001b[0m \tRan out of time, early stopping on iteration 2304. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17147)\u001b[0m \t[2269]\tvalid_set's rmse: 6023.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17631)\u001b[0m [1000]\tvalid_set's rmse: 6249.44\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17520)\u001b[0m [2000]\tvalid_set's rmse: 5978.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17520)\u001b[0m \tRan out of time, early stopping on iteration 2473. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17520)\u001b[0m \t[2473]\tvalid_set's rmse: 5962.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17631)\u001b[0m [2000]\tvalid_set's rmse: 6207.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17631)\u001b[0m \tRan out of time, early stopping on iteration 2418. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17631)\u001b[0m \t[2410]\tvalid_set's rmse: 6191.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m [1000]\tvalid_set's rmse: 6162.34\n",
      "\u001b[36m(_ray_fit pid=17828)\u001b[0m [1000]\tvalid_set's rmse: 6075.26\n",
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m [2000]\tvalid_set's rmse: 6125.28\n",
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m [3000]\tvalid_set's rmse: 6098.68\n",
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m [4000]\tvalid_set's rmse: 6073.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m \tRan out of time, early stopping on iteration 4418. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17829)\u001b[0m \t[4418]\tvalid_set's rmse: 6065.88\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/20 [02:23<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitted model: LightGBMDefault_BAG_L2/T1 ...\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-6041.1576\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t143.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t26.88s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 80.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \tEnsemble Weights: {'LightGBMDefault_BAG_L1/T1': 0.52, 'LightGBMDefault_BAG_L2/T1': 0.28, 'LightGBMXT_BAG_L2/T1': 0.16, 'LightGBMXT_BAG_L1/T1': 0.04}\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t-5870.1972\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t0.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m AutoGluon training complete, total runtime = 817.99s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 271.1 rows/s (121848 batch size)\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/dst78/AI_BootCamp/Apart_predict/autogluon_models/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=10831)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                       model  score_holdout    score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  LightGBMDefault_BAG_L2/T1   -5560.458705 -6041.157564  root_mean_squared_error       42.169502     411.968787  665.426589                 8.197424               26.881614         143.271300            2       True          5\n",
      "1        WeightedEnsemble_L3   -5661.586445 -5870.197235  root_mean_squared_error       48.076414     449.467366  806.504909                 0.002766                0.005936           0.352365            3       True          6\n",
      "2  LightGBMDefault_BAG_L1/T1   -5767.321394 -5945.983588  root_mean_squared_error       18.546474     180.963419  258.499575                18.546474              180.963419         258.499575            1       True          2\n",
      "3        WeightedEnsemble_L2   -5781.592500 -5923.178143  root_mean_squared_error       33.975391     385.095011  522.373516                 0.003314                0.007837           0.218227            2       True          3\n",
      "4       LightGBMXT_BAG_L2/T1   -6223.890248 -6386.136548  root_mean_squared_error       39.876224     422.579816  662.881245                 5.904147               37.492643         140.725956            2       True          4\n",
      "5       LightGBMXT_BAG_L1/T1   -6810.353353 -6962.244629  root_mean_squared_error       15.425603     204.123755  263.655714                15.425603              204.123755         263.655714            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t871s\t = DyStack   runtime |\t2729s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2729s\n",
      "AutoGluon will save models to \"/home/dst78/AI_BootCamp/Apart_predict/autogluon_models\"\n",
      "Train Data Rows:    1096631\n",
      "Train Data Columns: 46\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "\tAvailable Memory:                    15262.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1370.76 MB (9.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 9.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "Warning: dtype period[M] is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['연_월']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('period[M]', []) : 1 | ['연_월']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 24 | ['전용면적(㎡)', '계약일', '층', '건축년도', '해제사유발생일', ...]\n",
      "\t\t('int', [])                        :  1 | ['cluster']\n",
      "\t\t('object', [])                     : 17 | ['번지', '아파트명', '도로명', '거래유형', '중개사소재지', ...]\n",
      "\t\t('object', ['datetime_as_object']) :  3 | ['k-사용검사일-사용승인일', '단지승인일', '단지신청일']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 16 | ['번지', '아파트명', '도로명', '거래유형', '중개사소재지', ...]\n",
      "\t\t('float', [])                : 23 | ['전용면적(㎡)', '계약일', '층', '건축년도', '해제사유발생일', ...]\n",
      "\t\t('int', [])                  :  1 | ['cluster']\n",
      "\t\t('int', ['bool'])            :  2 | ['k-135㎡초과', '사용허가여부']\n",
      "\t\t('int', ['datetime_as_int']) : 15 | ['k-사용검사일-사용승인일', 'k-사용검사일-사용승인일.year', 'k-사용검사일-사용승인일.month', 'k-사용검사일-사용승인일.day', 'k-사용검사일-사용승인일.dayofweek', ...]\n",
      "\t4.6s = Fit runtime\n",
      "\t45 features in original data used to generate 57 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 347.22 MB (2.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.01s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'extra_trees': False, 'ag_args': {'name_suffix': 'Default'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 817.03s of the 2724.1s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0440bdff2b7b4b1badd0995592142025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.59% memory usage per fold, 66.37%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=16.59%)\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMXT_BAG_L1/T1 ...\n",
      "\t-6188.3124\t = Validation score   (-root_mean_squared_error)\n",
      "\t688.47s\t = Training   runtime\n",
      "\t735.63s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMDefault_BAG_L1 ... Tuning model for up to 817.03s of the 2034.88s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5fe18e325c4581b62dae69e31e34da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.55% memory usage per fold, 66.21%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=16.55%)\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMDefault_BAG_L1/T1 ...\n",
      "\t-5584.5134\t = Validation score   (-root_mean_squared_error)\n",
      "\t672.81s\t = Training   runtime\n",
      "\t657.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1361.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMDefault_BAG_L1/T1': 0.792, 'LightGBMXT_BAG_L1/T1': 0.208}\n",
      "\t-5539.4574\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 2 L2 models, fit_strategy=\"sequential\" ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L2 ... Tuning model for up to 612.4s of the 1360.82s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f23242a6274237b935363570e685c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.43% memory usage per fold, 69.73%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.43%)\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMXT_BAG_L2/T1 ...\n",
      "\t-5889.5428\t = Validation score   (-root_mean_squared_error)\n",
      "\t645.85s\t = Training   runtime\n",
      "\t639.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMDefault_BAG_L2 ... Tuning model for up to 612.4s of the 714.1s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaf34afe9be4dbea0c9aba6ba8df180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.46% memory usage per fold, 69.83%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.46%)\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.58% memory usage per fold, 70.33%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.58%)\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.49% memory usage per fold, 69.95%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.49%)\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.57% memory usage per fold, 70.30%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=6, gpus=0, memory=17.57%)\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBMDefault_BAG_L2/T1 ...\n",
      "\t-5743.2966\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.0s\t = Training   runtime\n",
      "\t4.54s\t = Validation runtime\n",
      "Fitted model: LightGBMDefault_BAG_L2/T2 ...\n",
      "\t-5722.8574\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.0s\t = Training   runtime\n",
      "\t5.84s\t = Validation runtime\n",
      "Fitted model: LightGBMDefault_BAG_L2/T3 ...\n",
      "\t-5761.6413\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.01s\t = Training   runtime\n",
      "\t7.55s\t = Validation runtime\n",
      "Fitted model: LightGBMDefault_BAG_L2/T4 ...\n",
      "\t-5741.6991\t = Validation score   (-root_mean_squared_error)\n",
      "\t437.88s\t = Training   runtime\n",
      "\t221.4s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 100.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMDefault_BAG_L1/T1': 0.55, 'LightGBMDefault_BAG_L2/T2': 0.2, 'LightGBMXT_BAG_L2/T1': 0.15, 'LightGBMXT_BAG_L1/T1': 0.1}\n",
      "\t-5508.6873\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2629.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 67.3 rows/s (137079 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/dst78/AI_BootCamp/Apart_predict/autogluon_models\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='autogluon_models',\n",
    "    eval_metric='root_mean_squared_error'\n",
    ").fit(\n",
    "    train_data=df,\n",
    "    time_limit=3600,\n",
    "    presets='best_quality',\n",
    "    hyperparameters={\n",
    "        'GBM': [\n",
    "            {\n",
    "                'extra_trees': True,\n",
    "                'ag_args': {'name_suffix': 'XT'}\n",
    "            },\n",
    "            {\n",
    "                'extra_trees': False,\n",
    "                'ag_args': {'name_suffix': 'Default'}\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    hyperparameter_tune_kwargs={\n",
    "        'num_trials': 20,\n",
    "        'scheduler': 'local',\n",
    "        'searcher': 'auto'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2052/2052 [02:38<00:00, 12.98it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = Entire_Preprocessing(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dst78/anaconda3/envs/Deep/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator KMeans from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/dst78/anaconda3/envs/Deep/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but KMeans was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 군집화 & 버스, 지하철 거리\n",
    "kmeans = joblib.load('kmeans_model.pkl')\n",
    "\n",
    "coords_exist = test_df[['좌표X', '좌표Y']].notna().all(axis=1)\n",
    "\n",
    "test_df.loc[coords_exist, 'cluster'] = kmeans.predict(\n",
    "   test_df.loc[coords_exist, ['좌표X', '좌표Y']]\n",
    ")\n",
    "\n",
    "test_df.loc[~coords_exist, 'cluster'] = np.nan\n",
    "\n",
    "test_apt_coords_rad = np.radians(\n",
    "   test_df.loc[coords_exist, ['좌표Y', '좌표X']].to_numpy()\n",
    ")\n",
    "\n",
    "if len(test_apt_coords_rad) > 0:\n",
    "   dist_bus_rad, _ = bus_tree.query(test_apt_coords_rad, k=1)\n",
    "   dist_sub_rad, _ = subway_tree.query(test_apt_coords_rad, k=1)\n",
    "   \n",
    "   test_df.loc[coords_exist, 'closest_bus'] = dist_bus_rad.flatten() * 6371.0\n",
    "   test_df.loc[coords_exist, 'closest_sub'] = dist_sub_rad.flatten() * 6371.0\n",
    "\n",
    "test_df.loc[~coords_exist, 'closest_bus'] = np.nan\n",
    "test_df.loc[~coords_exist, 'closest_sub'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 금리\n",
    "test_df['계약(연)'] = test_df['계약년월'] // 100\n",
    "test_df['계약(월)'] = test_df['계약년월'] % 100\n",
    "\n",
    "test_df['연_월'] = pd.PeriodIndex(year=test_df['계약(연)'], month=test_df['계약(월)'], freq='M')\n",
    "\n",
    "rates = {\n",
    "    \"2025-02\": 2.75, \"2024-11\": 3.00, \"2024-10\": 3.25, \"2023-01\": 3.50,\n",
    "    \"2022-11\": 3.25, \"2022-10\": 3.00, \"2022-08\": 2.50, \"2022-07\": 2.25,\n",
    "    \"2022-05\": 1.75, \"2022-04\": 1.50, \"2022-01\": 1.25, \"2021-11\": 1.00,\n",
    "    \"2021-08\": 0.75, \"2020-05\": 0.50, \"2020-03\": 0.75, \"2019-10\": 1.25,\n",
    "    \"2019-07\": 1.50, \"2018-11\": 1.75, \"2017-11\": 1.50, \"2016-06\": 1.25,\n",
    "    \"2015-06\": 1.50, \"2015-03\": 1.75, \"2014-10\": 2.00, \"2014-08\": 2.25,\n",
    "    \"2013-05\": 2.50, \"2012-10\": 2.75, \"2012-07\": 3.00, \"2011-06\": 3.25,\n",
    "    \"2011-03\": 3.00, \"2011-01\": 2.75, \"2010-11\": 2.50, \"2010-07\": 2.25,\n",
    "    \"2009-02\": 2.00, \"2009-01\": 2.50, \"2008-12\": 3.00, \"2008-11\": 4.00,\n",
    "    \"2008-10\": 5.00, \"2008-08\": 5.25, \"2007-08\": 5.00, \"2007-07\": 4.75,\n",
    "    \"2006-08\": 4.50\n",
    "}\n",
    "    \n",
    "test_df['금리'] = test_df['연_월'].apply(assign_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.replace('', np.nan)\n",
    "\n",
    "test_df[numeric_features] = scaler.transform(test_df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\n",
      "WARNING: Int features with nulls: ['cluster']\n"
     ]
    }
   ],
   "source": [
    "# Autogluon 예측\n",
    "pred = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = pred.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>229851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>236032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>222793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>175831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>381545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0  171520\n",
       "1  274593\n",
       "2  328479\n",
       "3  278501\n",
       "4  212132\n",
       "5  229851\n",
       "6  236032\n",
       "7  222793\n",
       "8  175831\n",
       "9  381545"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('AutoML_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
